{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_features.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def avg_and_std(scores):\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "# Função de avaliação sem K-Fold e PCA\n",
    "def evaluate_model(model, df, metric_funcs, test_size=0.2):\n",
    "    # Dividir os dados em features e target\n",
    "    X = df.iloc[:, 2:].values  # Features\n",
    "    y = df.iloc[:, 1].values   # Target\n",
    "    \n",
    "    # Dividir os dados em treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Treinar o modelo\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Obter previsões\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calcular e armazenar cada métrica\n",
    "    scores = {}\n",
    "    for metric_func in metric_funcs:\n",
    "        score = metric_func(y_test, y_pred)\n",
    "        scores[metric_func.__name__] = score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = 0\n",
    "best_params = {}\n",
    "\n",
    "for crit in [\"gini\", \"entropy\", \"log_loss\"]:\n",
    "    for n_est in range(25, 201, 25):\n",
    "        for m_depth in range(5, 106, 10):\n",
    "            for m_samples_leaf in range(5, 26, 5):\n",
    "                params = {\n",
    "                    'n_estimators': n_est,\n",
    "                    'max_depth': m_depth,\n",
    "                    'min_samples_leaf': m_samples_leaf,\n",
    "                    'criterion': crit\n",
    "                }\n",
    "                \n",
    "                rf_model = RandomForestClassifier(**params)\n",
    "                \n",
    "                score = evaluate_model(model=rf_model, df=df, metric_funcs=[roc_auc_score])\n",
    "                auc = score[\"roc_auc_score\"]\n",
    "                \n",
    "                if auc > best_auc:\n",
    "                    best_auc = auc\n",
    "                    best_params = params\n",
    "                    print(\"New best parameter combination found!\")\n",
    "                    for parameter, value in best_params.items():\n",
    "                        print(f\"\\t{parameter}: {value}\")\n",
    "                    print(f\"Best AUC: {best_auc * 100:.6f}%\\n\")\n",
    "                else:\n",
    "                    print(\"Parameter combination tested:\")\n",
    "                    for parameter, value in params.items():\n",
    "                        print(f\"\\t{parameter}: {value}\")\n",
    "                    print(f\"AUC: {auc * 100:.6f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameter combination:\")\n",
    "for parameter, value in best_params.items():\n",
    "    print(f\"\\t{parameter}: {value}\")\n",
    "print(f\"Best Average AUC: {best_auc * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
